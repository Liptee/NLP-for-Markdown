{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER для русского языка в spaCy 3: удобно и легко\n",
    "[Ссылка на статью](https://vc.ru/dev/299248-ner-dlya-russkogo-yazyka-v-spacy-3-udobno-i-legko?ysclid=l4wqnn7rur405121406)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Славянские языки, в том числе и русский, считаются довольно сложными для обработки. В основном, из-за богатой системы окончаний, свободного порядка слов и других морфологических и синтаксических явлений. Распознавание именованных сущностей (далее, NER) представляется трудной задачей для славянских языков, где синтаксические зависимости часто маркируются морфологическими чертами, нежели определенным порядком словоформ. Поэтому NER сложен для этих языков, в сравнении с германскими или романскими языками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER – популярная задача в сфере обработки естественного языка. Она заключается в распознавании именованных сущностей в тексте и определение их типов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За много лет подходы к NER для русского языка претерпели множество изменений. От rule-based тенденции, предложенной в 2004 году (Popov et al., 2004) до современных state-of-the-art решений с использованием encoder-decoder архитектуры и трансформеров. Появилось множество NLP библиотек, стремящихся к универсальности и простоте. На протяжении истории развития обработки языка, NLP инструментарии поддерживали несколько «основных» языков, но позже эта тенденция сместилась в сторону мультиязыковой обработки текста. Один из таких инструментариев – spaCy (бесплатная опенсоурс библиотека для усовершенствованной обработки естественного языка), который включает в себя предобученные модели для 18 разных языков. Для большинства задач spaCy (Honnibal & Montani, 2017) использует сверточные нейронные сети, но для NER используются transition-based подходы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зимой 2021 года у spaCy вышла новая версия – 3: появилась русская модель и возможность выбора стандартной модели для обучения в зависимости от наличия GPU. Для задачи NER для русского языка spaCy предлагает tok2vec и Multilingual BERT. Также, пользователь может самостоятельно выбрать предобученную модель из списка на HuggingFace.\n",
    "\n",
    "Попробуем натренировать NER на собственном датасете с использованием двух предложенных моделей. BSNLP Shared Task дает возможность загрузить тренировочные данные с аннотацией для нескольких славянских языков. Загрузим данные 2021 года. Они представлены в виде файлов с текстом и соответствующих лейблов следующего вида:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последний столбец нас не интересует, так как он относится к части задания Entity Matching, первый столбец – словоформа в тексте, второй – лемма, третий – именованная сущность. Чтобы работать с этими данными в spaCy, нужно привести их в специальный бинарный формат. Более подробней о нём – здесь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "Сначала находим нужную сущность из файлов с лейблами в тексте и записываем формат [index.start(), index.end(), label] в список, где index – индекс буквы в тексте, а label – аннотированная сущность. Затем очищаем полученный список от возможных пересечений индексов. Далее, с помощью встроенных функций spaCy приводим наш список в следующий формат: [«O», «O», «U-LOC», «O»], а потом кладем тексты и полученные тэги в привычный для spaCy doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def make_docs(folder, doc_list):\n",
    "    nlp = spacy.load('ru_core_news_lg')\n",
    "    out = 'out'\n",
    "\n",
    "    for filename in os.listdir(f'data/bsnlp2021_train_r1/raw/{folder}/ru'.format(folder=folder)):\n",
    "        df = pd.read_csv(f'data/bsnlp2021_train_r1/annotated/{folder}/ru/{filename}{out}'.format(folder=folder, filename=filename[:-3], out='out'), skiprows=1, header=None, sep='\\t', encoding='utf8',  error_bad_lines=False, engine='python')\n",
    "        f = open('data/bsnlp2021_train_r1/raw/{folder}/ru/{filename}'.format(folder=folder,filename=filename), \"r\", encoding='utf8')\n",
    "        list_words=df.iloc[:,0].tolist()\n",
    "        labels = df.iloc[:,2].tolist()\n",
    "        text = f.read()\n",
    "        entities=[]\n",
    "        for n in range(len(list_words)):\n",
    "            for m in re.finditer(list_words[n].strip(), text):\n",
    "                entities.append([m.start(), m.end(), labels[n]])\n",
    "\n",
    "        for f in range(len(entities)):\n",
    "            if len(entities[f])==3:\n",
    "                for s in range(f+1, len(entities)):\n",
    "                    if len(entities[s])==3 and len(entities[f])==3:\n",
    "                        if entities[f][0]==entities[s][0] or entities[f][1]==entities[s][1]:\n",
    "                            if (entities[f][1]-entities[f][0]) >= (entities[s][1]-entities[s][0]): \n",
    "                                entities.pop(s)\n",
    "                                entities.insert(s, (''))\n",
    "                            else:\n",
    "                                entities.pop(f)\n",
    "                                entities.insert(f, (''))\n",
    "                        if len(entities[s])==3 and len(entities[f])==3:\n",
    "                            if entities[f][0] in range(entities[s][0]+1, entities[s][1]):\n",
    "                                entities.pop(f)\n",
    "                                entities.insert(f, (''))\n",
    "                            elif entities[s][0] in range(entities[f][0]+1, entities[f][1]):\n",
    "                                entities.pop(s)\n",
    "                                entities.insert(s, (''))\n",
    "\n",
    "        entities_cleared = [i for i in entities if len(i)==3]\n",
    "        doc = nlp(text)\n",
    "        tags = offsets_to_biluo_tags(doc, entities_cleared)\n",
    "        #assert tags == [\"O\", \"O\", \"U-LOC\", \"O\"]\n",
    "        entities_x = biluo_tags_to_spans(doc, tags)\n",
    "        doc.ents = entities_x\n",
    "        doc_list.append(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('НИР_2-WuA8f2ou')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e6c9fab53fc3211fd37e0beae5e4569e950ef80c5ec8c35b734107b47e84f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
