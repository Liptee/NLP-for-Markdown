{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основы Natural Language Processing для текста\n",
    "[Ссылка на статью](https://habr.com/ru/company/Voximplant/blog/446738/?ysclid=l4se5aem4m570011661)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка естественного языка сейчас не используются разве что в совсем консервативных отраслях. В большинстве технологических решений распознавание и обработка «человеческих» языков давно внедрена: именно поэтому обычный IVR с жестко заданными опциями ответов постепенно уходит в прошлое, чатботы начинают все адекватнее общаться без участия живого оператора, фильтры в почте работают на ура и т.д. Как же происходит распознавание записанной речи, то есть текста? А вернее будет спросить, что лежит в основе соврменных техник распознавания и обработки? На это хорошо отвечает наш сегодняшний адаптированный перевод – под катом вас ждет лонгрид, который закроет пробелы по основам NLP. Приятного чтения!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое Natural Language Processing?\n",
    "Natural Language Processing (далее – NLP) – обработка естественного языка – подраздел информатики и AI, посвященный тому, как компьютеры анализируют естественные (человеческие) языки. NLP позволяет применять алгоритмы машинного обучения для текста и речи.\n",
    "\n",
    "Например, мы можем использовать NLP, чтобы создавать системы вроде распознавания речи, обобщения документов, машинного перевода, выявления спама, распознавания именованных сущностей, ответов на вопросы, автокомплита, предиктивного ввода текста и т.д.\n",
    "\n",
    "Сегодня у многих из нас есть смартфоны с распознаванием речи – в них используется NLP для того, чтобы понимать нашу речь. Также многие люди используют ноутбуки со встроенным в ОС распознаванием речи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-библиотека NLTK\n",
    "NLTK (Natural Language Toolkit) – ведущая платформа для создания NLP-программ на Python. У нее есть легкие в использовании интерфейсы для многих языковых корпусов, а также библиотеки для обработки текстов для классификации, токенизации, стемминга, разметки, фильтрации и семантических рассуждений. Ну и еще это бесплатный опенсорсный проект, который развивается с помощью коммьюнити.\n",
    "Мы будем использовать этот инструмент, чтобы показать основы NLP. Для всех последующих примеров я предполагаю, что NLTK уже импортирован; сделать это можно командой import nltk\n",
    "\n",
    "## Основы NLP для текста\n",
    "В этой статье мы рассмотрим темы:\n",
    "\n",
    "1. Токенизация по предложениям.\n",
    "2. Токенизация по словам.\n",
    "3. Лемматизация и стемминг текста.\n",
    "4. Стоп-слова.\n",
    "5. Регулярные выражения.\n",
    "6. Мешок слов.\n",
    "7. TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация по предложениям\n",
    "Токенизация (иногда – сегментация) по предложениям – это процесс разделения письменного языка на предложения-компоненты. Идея выглядит довольно простой. В английском и некоторых других языках мы можем вычленять предложение каждый раз, когда находим определенный знак пунктуации – точку.\n",
    "\n",
    "Но даже в английском эта задача нетривиальна, так как точка используется и в сокращениях. Таблица сокращений может сильно помочь во время обработки текста, чтобы избежать неверной расстановки границ предложений. В большинстве случаев для этого используются библиотеки, так что можете особо не переживать о деталях реализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать токенизацию предложений с помощью NLTK, можно воспользоваться методом `nltk.sent_tokenize`\n",
    "\n",
    "На выходе мы получим 3 отдельных предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon is one of the oldest known board games.',\n",
       " 'Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.',\n",
       " 'It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация по слоавам\n",
    "Токенизация (иногда – сегментация) по словам – это процесс разделения предложений на слова-компоненты. В английском и многих других языках, использующих ту или иную версию латинского алфавита, пробел – это неплохой разделитель слов.\n",
    "\n",
    "Тем не менее, могут возникнуть проблемы, если мы будем использовать только пробел – в английском составные существительные пишутся по-разному и иногда через пробел. И тут вновь нам помогают библиотеки.\n",
    "\n",
    "Давайте возьмем предложения из предыдущего примера и применим к ним метод `nltk.word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Backgammon',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'oldest',\n",
       " 'known',\n",
       " 'board',\n",
       " 'games',\n",
       " '.',\n",
       " 'Its',\n",
       " 'history',\n",
       " 'can',\n",
       " 'be',\n",
       " 'traced',\n",
       " 'back',\n",
       " 'nearly',\n",
       " '5,000',\n",
       " 'years',\n",
       " 'to',\n",
       " 'archeological',\n",
       " 'discoveries',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Middle',\n",
       " 'East',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'a',\n",
       " 'two',\n",
       " 'player',\n",
       " 'game',\n",
       " 'where',\n",
       " 'each',\n",
       " 'player',\n",
       " 'has',\n",
       " 'fifteen',\n",
       " 'checkers',\n",
       " 'which',\n",
       " 'move',\n",
       " 'between',\n",
       " 'twenty-four',\n",
       " 'points',\n",
       " 'according',\n",
       " 'to',\n",
       " 'the',\n",
       " 'roll',\n",
       " 'of',\n",
       " 'two',\n",
       " 'dice',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация и стемминг текста\n",
    "Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова. Лемматизация и стемминг преследуют цель привести все встречающиеся словоформы к одной, нормальной словарной форме.\n",
    "\n",
    "Лемматизация и стемминг – это частные случаи нормализации и они отличаются.\n",
    "\n",
    "Стемминг – это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов.\n",
    "\n",
    "Лемматизация – это более тонкий процесс, который использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – лемме.\n",
    "\n",
    "Отличие в том, что стеммер (конкретная реализация алгоритма стемминга – прим.переводчика) действует без знания контекста и, соответственно, не понимает разницу между словами, которые имеют разный смысл в зависимости от части речи. Однако у стеммеров есть и свои преимущества: их проще внедрить и они работают быстрее. Плюс, более низкая «аккуратность» может не иметь значения в некоторых случаях.\n",
    "\n",
    "**Примеры**:\n",
    "\n",
    "1. Слово good – это лемма для слова better. Стеммер не увидит эту связь, так как здесь нужно сверяться со словарем.\n",
    "2. Слово play – это базовая форма слова playing. Тут справятся и стемминг, и лемматизация.\n",
    "3. Слово meeting может быть как нормальной формой существительного, так и формой глагола to meet, в зависимости от контекста. В отличие от стемминга, лемматизация попробует выбрать правильную лемму, опираясь на контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоп-слова\n",
    "Стоп-слова – это слова, которые выкидываются из текста до/после обработки текста. Когда мы применяем машинное обучение к текстам, такие слова могут добавить много шума, поэтому необходимо избавляться от нерелевантных слов.\n",
    "\n",
    "Стоп-слова это обычно понимают артикли, междометия, союзы и т.д., которые не несут смысловой нагрузки. При этом надо понимать, что не существует универсального списка стоп-слов, все зависит от конкретного случая.\n",
    "\n",
    "В NLTK есть предустановленный список стоп-слов. Перед первым использованием вам понадобится его скачать: `nltk.download(“stopwords”)`. После скачивания можно импортировать пакет stopwords и посмотреть на сами слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WordListCorpusReader' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\muzzo\\OneDrive\\Рабочий стол\\НИР 2\\documentations\\nltk_tutorial.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/nltk_tutorial.ipynb#ch0000060?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/nltk_tutorial.ipynb#ch0000060?line=1'>2</a>\u001b[0m \u001b[39mlist\u001b[39;49m(stopwords)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'WordListCorpusReader' object is not iterable"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "list(stopwords)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2e6c9fab53fc3211fd37e0beae5e4569e950ef80c5ec8c35b734107b47e84f3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('НИР_2-WuA8f2ou')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
