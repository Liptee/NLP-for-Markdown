{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natasha\n",
    "[Ссылка на репозиторий](https://github.com/natasha/natasha) \n",
    "Наташа решает основные NLP задачи для русского языка: токенизация, сегментация по предложениям, word embedding, классификация частей речи, лемматизация, нормализация фраз, парсинг синтаксиса, нахождение именованных сущностей, извлечение фактов. Качество по каждой задаче аналогично или лучше, чему текущие результаты SOTA для русского языка. Наташа - не исследовательский проект, основные технологии созданы для производства. Мы уделили внимание размеру модели, использованию оперативной памяти и производительности. Модель запускается на процессоре, используя интерфейс Numpy. \n",
    "\n",
    "Наташа объединяет библиотеки проекта под одним удобным API:\n",
    "- **Razdel** - токен или сегментированное предложение.\n",
    "- **Navec** - компактный эмбеддинг русских слов.\n",
    "- **Slovnet** - современный инструмент на основе глубоко обучения для задач русскоязычного NLP, а также компактная модель для морфологии, синаксиса и нахождения именованных сущностей.\n",
    "- **Yargy** - аналог парсера Tomita для извлечения фактов.\n",
    "- **Ipymarkup** - NLP визуализатор для именованных сущностей и синтаксической маркировки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "Natasha supports Python 3.5+ and PyPy3:\n",
    "```bash\n",
    "$ pip install natasha\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "Импорт, инициализация модулей, построение `Doc` объекта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "text = 'Меркатто-Веккьо - торговый центр Флоренции. Антонио Пуччи, поэт, живший в XIV-м веке, так описал Меркато-Веккьо: \"Врачи, готовые исцелить любую хворь, торговцы тканями, продавцы свиных колбас и аптекари\". Во времена Римской Империи на месте Меркато был форум, и только в документах 1030 года он впервые назван рынком. Самым большим спросом здесь пользовались пшеница и шерсть. Однако если покупателям надоедало бродить по лавкам, они могли и развлечься. Иль-Панормита, поэт XV века, написал об этом так: \"Там, в глубине, стоит веселый бордель, который ты узнаешь по запаху\".'\n",
    "doc = Doc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "Разделение текста на токены и предложения. Определяет токены и предложения для `doc`. Использует `Razdel` внутри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=15, text='Меркатто-Веккьо'), DocToken(start=16, stop=17, text='-'), DocToken(start=18, stop=26, text='торговый'), DocToken(start=27, stop=32, text='центр'), DocToken(start=33, stop=42, text='Флоренции')]\n"
     ]
    }
   ],
   "source": [
    "doc.segment(segmenter)\n",
    "print(doc.tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocSent(stop=43, text='Меркатто-Веккьо - торговый центр Флоренции.', tokens=[...]), DocSent(start=44, stop=204, text='Антонио Пуччи, поэт, живший в XIV-м веке, так опи..., tokens=[...]), DocSent(start=205, stop=317, text='Во времена Римской Империи на месте Меркато был ф..., tokens=[...]), DocSent(start=318, stop=376, text='Самым большим спросом здесь пользовались пшеница ..., tokens=[...]), DocSent(start=377, stop=453, text='Однако если покупателям надоедало бродить по лавк..., tokens=[...])]\n"
     ]
    }
   ],
   "source": [
    "print(doc.sents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphology\n",
    "Для каждого токена извлекается богатая обозначеная морфология. Зависит от глубины сегментации. Определяет `pos` и `feats` свойства для `doc.tokens`. Использует **Slovnet morphology model** внутри.\n",
    "\n",
    "Используйте `morph.print()` чтобы визуализировать морфологическую разметку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=15, text='Меркатто-Веккьо', pos='PROPN', feats=<Inan,Nom,Masc,Sing>), DocToken(start=16, stop=17, text='-', pos='PUNCT'), DocToken(start=18, stop=26, text='торговый', pos='ADJ', feats=<Nom,Pos,Masc,Sing>), DocToken(start=27, stop=32, text='центр', pos='NOUN', feats=<Inan,Nom,Masc,Sing>), DocToken(start=33, stop=42, text='Флоренции', pos='PROPN', feats=<Inan,Gen,Fem,Sing>)]\n"
     ]
    }
   ],
   "source": [
    "doc.tag_morph(morph_tagger)\n",
    "print(doc.tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Меркатто-Веккьо PROPN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "                   - PUNCT\n",
      "            торговый ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
      "               центр NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "           Флоренции PROPN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n",
      "                   . PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc.sents[0].morph.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Лемматизирует каждый токен. Зависит от морфологической глубины. Определяет `lemma` свойства для `doc.tokens`. Использует **Pymorphy** внутри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=15, text='Меркатто-Веккьо', pos='PROPN', feats=<Inan,Nom,Masc,Sing>, lemma='меркатто-веккьо'), DocToken(start=16, stop=17, text='-', pos='PUNCT', lemma='-'), DocToken(start=18, stop=26, text='торговый', pos='ADJ', feats=<Nom,Pos,Masc,Sing>, lemma='торговый'), DocToken(start=27, stop=32, text='центр', pos='NOUN', feats=<Inan,Nom,Masc,Sing>, lemma='центр'), DocToken(start=33, stop=42, text='Флоренции', pos='PROPN', feats=<Inan,Gen,Fem,Sing>, lemma='флоренция')]\n"
     ]
    }
   ],
   "source": [
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "\n",
    "print(doc.tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax\n",
    "Для каждого предложения запуускает анализатор синтаксиса. Зависит от глубины сегментации. Определяет `id`, `head_id`, `rel` свойства для `doc.tokens`. Использует **Slovnet syntax model** внутри себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocToken(stop=15, text='Меркатто-Веккьо', id='1_1', head_id='1_4', rel='nsubj', pos='PROPN', feats=<Inan,Nom,Masc,Sing>, lemma='меркатто-веккьо'), DocToken(start=16, stop=17, text='-', id='1_2', head_id='1_4', rel='punct', pos='PUNCT', lemma='-'), DocToken(start=18, stop=26, text='торговый', id='1_3', head_id='1_4', rel='amod', pos='ADJ', feats=<Nom,Pos,Masc,Sing>, lemma='торговый'), DocToken(start=27, stop=32, text='центр', id='1_4', head_id='1_1', rel='appos', pos='NOUN', feats=<Inan,Nom,Masc,Sing>, lemma='центр'), DocToken(start=33, stop=42, text='Флоренции', id='1_5', head_id='1_4', rel='nmod', pos='PROPN', feats=<Inan,Gen,Fem,Sing>, lemma='флоренция')]\n"
     ]
    }
   ],
   "source": [
    "doc.parse_syntax(syntax_parser)\n",
    "print(doc.tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────► Меркатто-Веккьо nsubj\n",
      "│ ┌──► -               punct\n",
      "│ │ ┌► торговый        amod\n",
      "└─└─└─ центр           \n",
      "│ └──► Флоренции       nmod\n",
      "└────► .               punct\n"
     ]
    }
   ],
   "source": [
    "doc.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER\n",
    "Извлекает стандартные именованные сущности: имена, локации, организации. Зависит от глубины сегментации. Определяет `spans` свойства для `doc`. Использует внутри **Slovnet NER mdeol**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.tag_ner(ner_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocSpan(stop=15, type='LOC', text='Меркатто-Веккьо', tokens=[...]), DocSpan(start=33, stop=42, type='LOC', text='Флоренции', tokens=[...]), DocSpan(start=44, stop=57, type='PER', text='Антонио Пуччи', tokens=[...]), DocSpan(start=97, stop=111, type='PER', text='Меркато-Веккьо', tokens=[...]), DocSpan(start=216, stop=231, type='LOC', text='Римской Империи', tokens=[...])]\n"
     ]
    }
   ],
   "source": [
    "print(doc.spans[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Меркатто-Веккьо - торговый центр Флоренции. Антонио Пуччи, поэт, \n",
      "LOC────────────                  LOC──────  PER──────────        \n",
      "живший в XIV-м веке, так описал Меркато-Веккьо: \"Врачи, готовые \n",
      "                                PER───────────                  \n",
      "исцелить любую хворь, торговцы тканями, продавцы свиных колбас и \n",
      "аптекари\". Во времена Римской Империи на месте Меркато был форум, и \n",
      "                      LOC────────────          PER────              \n",
      "только в документах 1030 года он впервые назван рынком. Самым большим \n",
      "спросом здесь пользовались пшеница и шерсть. Однако если покупателям \n",
      "надоедало бродить по лавкам, они могли и развлечься. Иль-Панормита, \n",
      "                                                     PER──────────  \n",
      "поэт XV века, написал об этом так: \"Там, в глубине, стоит веселый \n",
      "бордель, который ты узнаешь по запаху\".\n"
     ]
    }
   ],
   "source": [
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity normalization\n",
    "Для каждой ИС применима процедура нормализации. Зависит от ИС, морфологии и синаксичкской глубины. Определяет `normal` свойство для `doc.spans`.\n",
    "\n",
    "Оно не просто лемматизирует каждый токен внутри `span`, чтобы \"Организации украинских националистов\" стала \"Организация украинские националисты\". Natasha использует синтаксическую взаимосвязь для корректного вывода \"Организация украинских националистов\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocSpan(stop=15, type='LOC', text='Меркатто-Веккьо', tokens=[...], normal='Меркатто-Веккьо'), DocSpan(start=33, stop=42, type='LOC', text='Флоренции', tokens=[...], normal='Флоренция'), DocSpan(start=44, stop=57, type='PER', text='Антонио Пуччи', tokens=[...], normal='Антонио Пуччи'), DocSpan(start=97, stop=111, type='PER', text='Меркато-Веккьо', tokens=[...], normal='Меркато-Веккьо'), DocSpan(start=216, stop=231, type='LOC', text='Римской Империи', tokens=[...], normal='Римская Империя')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Флоренции': 'Флоренция', 'Римской Империи': 'Римская Империя'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "print(doc.spans[:5])\n",
    "{_.text: _.normal for _ in doc.spans if _.text != _.normal}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity pasrsing\n",
    "Парсинг ИС преобразуется в имя, фамилию и отчество. Зависит от шага **NER**. Определяет `fact` свойство для `doc.spans`. Использует **Yargy-parser** внутри себя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocSpan(start=6, stop=13, type='LOC', text='Израиля', tokens=[...], normal='Израиль'), DocSpan(start=17, stop=24, type='LOC', text='Украине', tokens=[...], normal='Украина'), DocSpan(start=25, stop=35, type='PER', text='Йоэль Лион', tokens=[...], normal='Йоэль Лион', fact=DocFact(slots=[...])), DocSpan(start=89, stop=106, type='LOC', text='Львовской области', tokens=[...], normal='Львовская область'), DocSpan(start=152, stop=158, type='LOC', text='России', tokens=[...], normal='Россия')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Йоэль Лион': {'first': 'Йоэль', 'last': 'Лион'},\n",
       " 'Степан Бандера': {'first': 'Степан', 'last': 'Бандера'},\n",
       " 'Петр Порошенко': {'first': 'Петр', 'last': 'Порошенко'},\n",
       " 'Бандера': {'last': 'Бандера'},\n",
       " 'Виктор Ющенко': {'first': 'Виктор', 'last': 'Ющенко'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for span in doc.spans:\n",
    "    if span.type == PER:\n",
    "        span.extract_fact(names_extractor)\n",
    "\n",
    "print(doc.spans[:5])\n",
    "{_.normal: _.fact.as_dict for _ in doc.spans if _.type == PER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "- [Examples with description + reference](https://nbviewer.org/github/natasha/natasha/blob/master/docs.ipynb)\n",
    "- [Natasha section in longread on Natasha project](https://habr.com/ru/post/516098/)\n",
    "- [Natasha section of Datafest 2020 talk](https://www.youtube.com/watch?v=-7XT_U6hVvk&t=951s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('НИР_2-WuA8f2ou')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e6c9fab53fc3211fd37e0beae5e4569e950ef80c5ec8c35b734107b47e84f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
