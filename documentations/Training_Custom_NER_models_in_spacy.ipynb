{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевод статьи: https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Содержание\n",
    "1. Введение в распознование именованных сущностей\n",
    "2. Необходимость настройки NER\n",
    "3. Обновление распознователя именованных сущностей\n",
    "4. Формат выборки для обучения\n",
    "5. Обучение NER модели\n",
    "6. Предсказание модели на новом тексте\n",
    "7. Как обучить NER с чистого листа в spaCy\n",
    "8. Обучение новой сущности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в распознание именованных сущностей\n",
    "**spaCy** - это библиотека в области NLP с открытым исходным кодом. Широко используемая из-за своей гибкости и продвинутых инструментов. Перед тем как углубляться непосредственно в задачу NER в spaCy, давйте быстро поймем чтоже такое задач **распознование именованных сущностей**.\n",
    "\n",
    "**Named Entity Recognition** - это классическая задача NLP по нахождению обсуждаемых объектов в тексте.\n",
    "\n",
    "**Модель NER** - это модель которая способна выполнять эту задачу. Она должна быть способна идентифицировать такие сущности как \"Америка\", \"Эмили\", \"Лондон\" и т.д., и маркировать их соответствующе `PERSON`, `LOCATION` и прочее. Этот очень полезный инструмент помогает в поиске информации. В spaCy NER осуществляется `pipeline` компонентом `ner`. Большинство моделей по умолчанию используют в себе `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a spacy model and check if it has ner\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае если ваша модель не имеет `ner`, вы можете добавить его используя метод `nlp.add_pipe()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Необходимость настройки NER\n",
    "Как вы видите, spaCy имеет встроенный pipeline `ner` для распознования имен. Хоть он довольно хорошо справляется со своей работой, но не всегда его точности может хватать для вашей задачи. Порой, слово обозначается как `PERSON` или `ORG` в зависимости от контекста. Также, иногда категории, которую вы хотите извлекать не предусмотрена в spaCy.\n",
    "\n",
    "Давайте взглянем как стандратная модель NER справится со статье о компании в сфере цифровой торговли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfoming NER on E-commerce article\n",
    "\n",
    "article_text = \"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
    "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
    "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
    "Amazon – One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
    "\n",
    "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
    "\n",
    "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
    "\n",
    "ShopClues – Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
    "\n",
    "Paytm Mall – To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace – Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app – Paytm. \n",
    "\n",
    "Reliance Retail – Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
    "Big Basket – India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services – express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
    "\n",
    "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
    "\n",
    "Digital Mall of Asia – Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
    "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India GPE\n",
      "one CARDINAL\n",
      "Indian NORP\n",
      "USD 84 billion MONEY\n",
      "2021 DATE\n",
      "USD 24 billion MONEY\n",
      "2017 DATE\n",
      "India GPE\n",
      "Philip PERSON\n",
      "12% PERCENT\n",
      "2017 DATE\n",
      "22-25% PERCENT\n",
      "2021 DATE\n",
      "India GPE\n",
      "Amazon ORG\n",
      "One CARDINAL\n",
      "Amazon ORG\n",
      "Amazon ORG\n",
      "Amazon Music Limited ORG\n",
      "2007 DATE\n",
      "Flipkart ORG\n",
      "Indian NORP\n",
      "Amazon ORG\n",
      "Walmart ORG\n",
      "one CARDINAL\n",
      "US GPE\n",
      "Amazon ORG\n",
      "2010 DATE\n",
      "Snapdeal PERSON\n",
      "2011 DATE\n",
      "more than 3 CARDINAL\n",
      "India GPE\n",
      "over 30 million CARDINAL\n",
      "800+ CARDINAL\n",
      "over 125,000 CARDINAL\n",
      "Indian NORP\n",
      "recent years DATE\n",
      "Freecharge ORG\n",
      "Unicommerce ORG\n",
      "Indian NORP\n",
      "ShopClues ORG\n",
      "July 2011 DATE\n",
      "Gurugram ORG\n",
      "INR ORG\n",
      "1.1 billion CARDINAL\n",
      "Nexus Venture Partners ORG\n",
      "Tiger Global PERSON\n",
      "Helion Ventures ORG\n",
      "nine CARDINAL\n",
      "Paytm PERSON\n",
      "Paytm Mall PERSON\n",
      "third ORDINAL\n",
      "Indian NORP\n",
      "Reliance ORG\n",
      "Reliance ORG\n",
      "India GPE\n",
      "Big Basket ORG\n",
      "two CARDINAL\n",
      "One CARDINAL\n",
      "Grofers ORG\n",
      "2013 DATE\n",
      "the last 5 years DATE\n",
      "atta ORG\n",
      "daily DATE\n",
      "India GPE\n",
      "Indian NORP\n",
      "2020 DATE\n",
      "Digital Mall FAC\n",
      "Asia LOC\n",
      "Yokeasia Malls ORG\n",
      "zero-commission MONEY\n",
      "monthly DATE\n",
      "DMA ORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(article_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на выход. Заметим, что FLIPKART был идентифицирован как `PERSON`, хотя должен быть `ORG`(на момент статьи наверное так было. Сейчас он обозначается как `ORG`). \"Walmart\" также был опознан неверно как `LOC`, а по контесту ему следует быть `ORG`. Есть другие примеры неверного распознования.\n",
    "\n",
    "В таких случаях вы можете столкнуться с необходимостью улучшить и дообучить NER по необходимости конкретного контекста. Следующий раздел покажет вам как это сделать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обновление распознователя именованных сущностей\n",
    "В прошлом разделе, вы могли увидеть почему нам нужно дообучить модель NER. Давайте продолжим и посмотрим как это сделать.\n",
    "\n",
    "Давайте предположим, что вы распологаете разнообразными текстами о клиентских отзывах и компаниях. Наша задача убидиться что NER распознает компании, как `ORG`, а не как `PERSON`, помечает продукцию как `PRODUCT` и т.д.\n",
    "\n",
    "Для этого вам нужно предоставить модели примеры на которых в будущем NER сможет учиться.\n",
    "\n",
    "Чтобы это сделать, давайте использовать уже предобученную модель spaCy и улучшим ее новыми примерами.\n",
    "\n",
    "Сначала, давайте загрузим уже существующую модель с компонентом `ner`. Затем, получим NER с помощью метода `get_pipe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-existing spacy model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобу улучшить предобученную модель новыми примерами, вы должны предоставить множество примеров, чтобы существенно улучшить систему - несколько сотен это хороший старт, но чем больше, тем лучше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Формат выборки для обучения\n",
    "spaCy принмиает тренировочные данные как список кортежей.\n",
    "\n",
    "Каждый кортеж должен содердать текст и словарь. Словарь должен содержать началльные и конечные индексы именованной сущности в тексте, и категорию обозначаемой сущности.\n",
    "\n",
    "Например: `(\"Walmart is a leading e-commerce company\", {\"entities\": [(0,7, \"ORG\")]})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код выше показывает вам чистый формат тренировчных данных. Вам нужно добавить эти лейблы к использованию ner через `ner.add_label()` pipeline. Код ниже демонстрирует это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь время обучать NER с помощью этих примеров. Но перед обучением, помните, что модель крмое `ner` имеет другие компоненты pipeline. Эти компоненты не должны пострадать при обучении.\n",
    "\n",
    "Перед обученем следует отключить другие компоненты pipeline с помощью метода `nlp.disable_pipes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pipeline components you don't need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Обучение модели NER\n",
    "Для начала, давайте поймем идеи внедрение перед кодом.\n",
    "1. Чтобы обучить `ner` модель должна быть зациклена достаточным количеством итераций. Если в вашем обучении 5 или 6 итераций, это может быть недостаточно эффективно.\n",
    "2. Перед каждой итерацией хорошей практикой будет перемешивание примеров, используя `random.shuffle()`. Это обеспечит что модель не будет обращать внимание на порядок примеров в своем обучении.\n",
    "3. Данные для обучения обычно передаются группами.\n",
    "\n",
    "Вы можете вызвать функцию `minibatch()`, которая вернет вам данные укомплектованные в пакетах. Функция принимает параметр `size`, который указывает на размер пакета. Вы можете использовать функцию `compounding` для генерации бесконечного ряда составляющих значений.(???). Функция `compounding()` принимает три значения на входе `start`(первое целочисленное значение), `stop` (максимально генерируемое значение) и `compound`. Это значение хранимое в `compound` является компаундирующим фактором для ряда. Если вы не понимаете о чем речь, зацените [эту](https://spacy.io/api/top-level#util) статью.\n",
    "\n",
    "Для каждой итерации, модель `ner` обновляется с помощью `nlp.update()`. Параметры для `nlp.update()`:\n",
    "- `docs`: ожидает количество пакетов текста на вход. Вы можете пропустить каждый пакет используя метод `zip`, который вернет вам пакеты текста и аннотаций.\n",
    "- `golds`: вы можете передать, которые пройдут через zip-метод внутри функции.\n",
    "- `drop`: отражает процент отсева.\n",
    "- `losses`: словарь для удержания потерь по кажому компоненту pipeline. Создайте пустой словарь и передайте его.\n",
    "\n",
    "Для каждого слова `update()` создает предсказание. Затем он проверяет аннотации и сверяется с ними. Если предсказание неверно, веса регулируются, для большей точности в следующей попытке.\n",
    "\n",
    "Наконец, все обучение проводится в рамках модель NLP с отключенным pipeline, чтобы предотвратить участие других компонентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оригинальный пример работает для старой версии spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\muzzo\\OneDrive\\Рабочий стол\\НИР 2\\documentations\\Training_Custom_NER_models_in_spacy.ipynb Ячейка 20\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m batches:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=17'>18</a>\u001b[0m     texts, annotations \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=18'>19</a>\u001b[0m     nlp\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=19'>20</a>\u001b[0m         texts, \u001b[39m# batch of texts\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=20'>21</a>\u001b[0m         annotations, \u001b[39m# batch of annotations\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=21'>22</a>\u001b[0m         drop \u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m, \u001b[39m# dropout - make it harder to memorise data\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=22'>23</a>\u001b[0m         losses\u001b[39m=\u001b[39;49mlosses,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=23'>24</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/muzzo/OneDrive/%D0%A0%D0%B0%D0%B1%D0%BE%D1%87%D0%B8%D0%B9%20%D1%81%D1%82%D0%BE%D0%BB/%D0%9D%D0%98%D0%A0%202/documentations/Training_Custom_NER_models_in_spacy.ipynb#ch0000021?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLosses: \u001b[39m\u001b[39m{\u001b[39;00mlosses\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\language.py:1142\u001b[0m, in \u001b[0;36mLanguage.update\u001b[1;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[39m\"\"\"Update the models in the pipeline.\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \n\u001b[0;32m   1126\u001b[0m \u001b[39mexamples (Iterable[Example]): A batch of examples\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1139\u001b[0m \u001b[39mDOCS: https://spacy.io/api/language#update\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[39mif\u001b[39;00m _ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE989)\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m losses \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1144\u001b[0m     losses \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: [E989] `nlp.update()` was called with two positional arguments. This may be due to a backwards-incompatible change to the format of the training data in spaCy 3.0 onwards. The 'update' function should now be called with a batch of Example objects, instead of `(text, annotation)` tuples. "
     ]
    }
   ],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "    # Training for 30 iterations\n",
    "    for iterations in range(30):\n",
    "\n",
    "        # shuffling examples before every iteration\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts, # batch of texts\n",
    "                annotations, # batch of annotations\n",
    "                drop = 0.5, # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "            print(f\"Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I reached Chennai yesterday.\" with entities \"[(19, 28, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a screwdriver from our neighbour\" with entities \"[(12, 22, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a new Washer\" with entities \"[(16, 22, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 1.2440795134698972}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a camera\" with entities \"[(12, 18, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 1.244079758483601}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a fancy dress\" with entities \"[(18, 23, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I got my clock fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I repaired my computer\" with entities \"[(15, 23, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 2.503857192607865}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered from Swiggy\" with entities \"[(24, 29, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered a book from Amazon\" with entities \"[(24, 32, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I got my truck fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 4.014475314118136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a old table\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a tent for our trip\" with entities \"[(12, 16, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 4.014475987299401}\n",
      "Losses: {'ner': 0.01703479488062577}\n",
      "Losses: {'ner': 0.01703479488328802}\n",
      "Losses: {'ner': 0.0170361193126194}\n",
      "Losses: {'ner': 0.017036212491287308}\n",
      "Losses: {'ner': 0.017036212625621945}\n",
      "Losses: {'ner': 0.0001516827455703087}\n",
      "Losses: {'ner': 0.00015168274702582872}\n",
      "Losses: {'ner': 0.00015168275051086918}\n",
      "Losses: {'ner': 0.023547553961130764}\n",
      "Losses: {'ner': 0.023547576293629476}\n",
      "Losses: {'ner': 1.9120081825983253e-08}\n",
      "Losses: {'ner': 1.9272496372519103e-08}\n",
      "Losses: {'ner': 1.9275001955665587e-08}\n",
      "Losses: {'ner': 3.026829897980295e-08}\n",
      "Losses: {'ner': 6.28261193716378e-08}\n",
      "Losses: {'ner': 9.380849382321637e-07}\n",
      "Losses: {'ner': 1.0732864770392192e-06}\n",
      "Losses: {'ner': 0.00013742051183593897}\n",
      "Losses: {'ner': 0.00013903865715629484}\n",
      "Losses: {'ner': 0.00013903865715643422}\n",
      "Losses: {'ner': 1.5702018525498085e-13}\n",
      "Losses: {'ner': 2.071511476035037e-11}\n",
      "Losses: {'ner': 3.304516132389817e-11}\n",
      "Losses: {'ner': 3.5582249745350976e-11}\n",
      "Losses: {'ner': 3.588875049773353e-06}\n",
      "Losses: {'ner': 9.415830534733658e-11}\n",
      "Losses: {'ner': 1.9409135311677e-06}\n",
      "Losses: {'ner': 2.0015325989132757e-06}\n",
      "Losses: {'ner': 3.649257006305589e-05}\n",
      "Losses: {'ner': 0.00010107202144468257}\n",
      "Losses: {'ner': 0.18141133731389464}\n",
      "Losses: {'ner': 0.18141133731486286}\n",
      "Losses: {'ner': 0.18141133746119684}\n",
      "Losses: {'ner': 0.18141133810163307}\n",
      "Losses: {'ner': 0.18141133817584268}\n",
      "Losses: {'ner': 1.721837516261936e-19}\n",
      "Losses: {'ner': 1.9378368204980805}\n",
      "Losses: {'ner': 1.937836820498103}\n",
      "Losses: {'ner': 1.9378368255077838}\n",
      "Losses: {'ner': 1.9378368255077838}\n",
      "Losses: {'ner': 2.8703311614224214e-12}\n",
      "Losses: {'ner': 0.0005025997557718363}\n",
      "Losses: {'ner': 0.0005025997557718372}\n",
      "Losses: {'ner': 0.0005026015335198458}\n",
      "Losses: {'ner': 0.0005026015335625484}\n",
      "Losses: {'ner': 0.0014512694906444557}\n",
      "Losses: {'ner': 0.0014512695081312739}\n",
      "Losses: {'ner': 0.014339856757671727}\n",
      "Losses: {'ner': 0.01433985675767176}\n",
      "Losses: {'ner': 0.014339856757671772}\n",
      "Losses: {'ner': 8.530955748827854e-12}\n",
      "Losses: {'ner': 2.8028142367494096e-11}\n",
      "Losses: {'ner': 1.6548878956633387e-10}\n",
      "Losses: {'ner': 1.655206102860152e-10}\n",
      "Losses: {'ner': 1.6552061034402423e-10}\n",
      "Losses: {'ner': 1.0701745661425706e-13}\n",
      "Losses: {'ner': 1.0701759635927357e-13}\n",
      "Losses: {'ner': 1.2251642343417688e-08}\n",
      "Losses: {'ner': 1.2251642384722028e-08}\n",
      "Losses: {'ner': 0.005000712494459839}\n",
      "Losses: {'ner': 2.684179128862307e-19}\n",
      "Losses: {'ner': 1.4062383714931155e-13}\n",
      "Losses: {'ner': 8.507512254457319e-13}\n",
      "Losses: {'ner': 1.075206473985752e-12}\n",
      "Losses: {'ner': 1.269690793663326e-12}\n",
      "Losses: {'ner': 2.884313536731848e-13}\n",
      "Losses: {'ner': 0.0679278790656255}\n",
      "Losses: {'ner': 0.06792787906755558}\n",
      "Losses: {'ner': 1.4260626256191244}\n",
      "Losses: {'ner': 1.4260626256191442}\n",
      "Losses: {'ner': 4.512819960648888e-16}\n",
      "Losses: {'ner': 4.684010415500147e-16}\n",
      "Losses: {'ner': 1.4170814956479176e-14}\n",
      "Losses: {'ner': 6.456982438869717e-14}\n",
      "Losses: {'ner': 1.9941685200235024}\n",
      "Losses: {'ner': 2.307730965087137e-13}\n",
      "Losses: {'ner': 1.1199814258180524e-08}\n",
      "Losses: {'ner': 1.1217723628809534e-08}\n",
      "Losses: {'ner': 1.1530138225339731e-08}\n",
      "Losses: {'ner': 1.171265669440559e-08}\n",
      "Losses: {'ner': 3.8299514635731876e-16}\n",
      "Losses: {'ner': 6.122588976148613e-16}\n",
      "Losses: {'ner': 1.0173361553573585e-14}\n",
      "Losses: {'ner': 1.4529304505966892e-12}\n",
      "Losses: {'ner': 1.4666270838117528e-12}\n",
      "Losses: {'ner': 0.031726386398083804}\n",
      "Losses: {'ner': 0.03172638647084706}\n",
      "Losses: {'ner': 0.031726386471927694}\n",
      "Losses: {'ner': 0.03172638647194131}\n",
      "Losses: {'ner': 0.03172638647194131}\n",
      "Losses: {'ner': 4.385171470023455e-09}\n",
      "Losses: {'ner': 2.986704588171748e-08}\n",
      "Losses: {'ner': 2.9899017000219326e-08}\n",
      "Losses: {'ner': 2.98990454254241e-08}\n",
      "Losses: {'ner': 2.9899045426316354e-08}\n",
      "Losses: {'ner': 0.03285981359380985}\n",
      "Losses: {'ner': 0.03285981359381205}\n",
      "Losses: {'ner': 0.032859813603266654}\n",
      "Losses: {'ner': 0.03285981362999457}\n",
      "Losses: {'ner': 0.032859813631129095}\n",
      "Losses: {'ner': 9.988012075743107e-13}\n",
      "Losses: {'ner': 1.0507497451027458e-12}\n",
      "Losses: {'ner': 4.531837658188588e-12}\n",
      "Losses: {'ner': 2.1085128331330118e-06}\n",
      "Losses: {'ner': 2.1085860022707857e-06}\n",
      "Losses: {'ner': 1.3561781689083705e-15}\n",
      "Losses: {'ner': 7.222415142808362e-09}\n",
      "Losses: {'ner': 7.2224152942280355e-09}\n",
      "Losses: {'ner': 7.223375850065875e-09}\n",
      "Losses: {'ner': 7.223375852127401e-09}\n",
      "Losses: {'ner': 9.506331660973563e-11}\n",
      "Losses: {'ner': 2.4615974065500693e-10}\n",
      "Losses: {'ner': 7.391851886638778e-10}\n",
      "Losses: {'ner': 9.802509671499798e-10}\n",
      "Losses: {'ner': 9.802509673112496e-10}\n",
      "Losses: {'ner': 7.273590128733351e-11}\n",
      "Losses: {'ner': 7.27359023802395e-11}\n",
      "Losses: {'ner': 7.765955308597556e-11}\n",
      "Losses: {'ner': 7.765968670993402e-11}\n",
      "Losses: {'ner': 8.402269274175984e-11}\n",
      "Losses: {'ner': 4.846044569905441e-14}\n",
      "Losses: {'ner': 1.8831317094420312e-10}\n",
      "Losses: {'ner': 1.883131716544354e-10}\n",
      "Losses: {'ner': 2.6110862733226173e-07}\n",
      "Losses: {'ner': 2.6110907359410935e-07}\n",
      "Losses: {'ner': 2.3128862368669137e-07}\n",
      "Losses: {'ner': 2.3128864827302995e-07}\n",
      "Losses: {'ner': 2.3129085332544483e-07}\n",
      "Losses: {'ner': 2.312908533709759e-07}\n",
      "Losses: {'ner': 2.3181479533874665e-07}\n",
      "Losses: {'ner': 1.2581102746205142e-07}\n",
      "Losses: {'ner': 1.258110274844506e-07}\n",
      "Losses: {'ner': 1.315402396106875e-07}\n",
      "Losses: {'ner': 0.2790842477607154}\n",
      "Losses: {'ner': 0.2790842477607173}\n",
      "Losses: {'ner': 9.98892677687751e-12}\n",
      "Losses: {'ner': 8.203826825602277e-08}\n",
      "Losses: {'ner': 8.239594519007186e-08}\n",
      "Losses: {'ner': 8.239594993920403e-08}\n",
      "Losses: {'ner': 8.239594994002276e-08}\n",
      "Losses: {'ner': 1.994378089904786}\n",
      "Losses: {'ner': 1.9943780899047876}\n",
      "Losses: {'ner': 2.006551978178325}\n",
      "Losses: {'ner': 2.006551978263911}\n",
      "Losses: {'ner': 2.0065538070567155}\n"
     ]
    }
   ],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "    # Training for 30 iterations\n",
    "    for iterations in range(30):\n",
    "\n",
    "        # shuffling examples before every iteration\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "\n",
    "        #batch up the examples using spaCy's minibatch\n",
    "        for batch in spacy.util.minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001)):\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                # Update the model\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    losses=losses,\n",
    "                    drop=0.5\n",
    "            )\n",
    "            print(f\"Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Тестирование модели на новом тексте\n",
    "Теперь обучение NER завершено. Вы можжете протестировать работу NER. Если вы не удовлетворены результами, добавьте больше примеров в датасет и попробуйте снова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: []\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I was driving a Alto\")\n",
    "print(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В статье у автора получилось извлечь \"Alto\" в категорию `PRODUCT`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель не запоминает примеры обучения. Она должна извлечь из них веса и быть способна обобщить их для новых примеров.\n",
    "\n",
    "Когда вы получите результаты модели, которые вас удовлетворят, сохраните новую модель.\n",
    "\n",
    "После сохранения, вы можете загрузить модель из директории в любое время используя `spacy.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to  spacy_models\n",
      "Loading from  spacy_models\n",
      "Entities [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to directory\n",
    "output_dir = Path('spacy_models')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to \", output_dir)\n",
    "\n",
    "#Load the saved model and predict\n",
    "print(\"Loading from \", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\")\n",
    "print(f\"Entities {[(ent.text, ent.label_) for ent in doc.ents]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Как обучить NER с нуля в spaCy\n",
    "Если вы не хотите использовать пред-обученную модель, вы можете создать свою пустую модель с помощью `spacy.blank()`, указав идентификатор языка. Для создания пустой модели на основе английского вы должны передать \"en\".\n",
    "\n",
    "После этого, основные шаги для обучения схожи. Ключевые моменты, которые стоит помнить:\n",
    "1. Так как модель пуста, она не имеет никаких элементов pipeline. Вы должны добавить `ner` компонент с помощью `add_pipe()`.\n",
    "2. Вам не нужно отключать другие компоненты pipeline соответственно.\n",
    "3. Вам понадобится куда более объемный набор данных в таком случае.\n",
    "4. Перед обучением настройте новую модель с помощью `nlp.begin_training()`\n",
    "\n",
    "Код ниже показывает шаги по созданию новой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<thinc.optimizers.Optimizer at 0x22c07ee6a20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train NER from a blank spacy model\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe('ner')\n",
    "nlp.begin_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После это вы можете делать теже самые процедуры, что и с прошлой пред-обученной моделью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I repaired my computer\" with entities \"[(15, 23, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a tent for our trip\" with entities \"[(12, 16, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a new Washer\" with entities \"[(16, 22, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I reached Chennai yesterday.\" with entities \"[(19, 28, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 13.813345491886139}\n",
      "Losses: {'ner': 24.91245061904192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a old table\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered a book from Amazon\" with entities \"[(24, 32, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a screwdriver from our neighbour\" with entities \"[(12, 22, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I got my clock fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered from Swiggy\" with entities \"[(24, 29, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a camera\" with entities \"[(12, 18, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 37.98668548837304}\n",
      "Losses: {'ner': 46.05652277544141}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I got my truck fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\muzzo\\.virtualenvs\\НИР_2-WuA8f2ou\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a fancy dress\" with entities \"[(18, 23, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 48.97552725486457}\n",
      "Losses: {'ner': 3.8638777513697278}\n",
      "Losses: {'ner': 5.431040444876999}\n",
      "Losses: {'ner': 8.793471081529788}\n",
      "Losses: {'ner': 12.456545059123897}\n",
      "Losses: {'ner': 12.464889374675469}\n",
      "Losses: {'ner': 1.9791981454558565}\n",
      "Losses: {'ner': 5.951718816328161}\n",
      "Losses: {'ner': 7.84543448425255}\n",
      "Losses: {'ner': 9.226640014595853}\n",
      "Losses: {'ner': 12.924371926896573}\n",
      "Losses: {'ner': 3.2098916093455774}\n",
      "Losses: {'ner': 5.2045615192908405}\n",
      "Losses: {'ner': 8.083584892926336}\n",
      "Losses: {'ner': 9.321083490139491}\n",
      "Losses: {'ner': 11.296569504956622}\n",
      "Losses: {'ner': 9.667202684220891e-05}\n",
      "Losses: {'ner': 2.236825981134593}\n",
      "Losses: {'ner': 5.293663625170484}\n",
      "Losses: {'ner': 7.817601093971397}\n",
      "Losses: {'ner': 9.743123472353533}\n",
      "Losses: {'ner': 0.5370544454512549}\n",
      "Losses: {'ner': 3.818505804082564}\n",
      "Losses: {'ner': 5.252454564604454}\n",
      "Losses: {'ner': 5.252569901513592}\n",
      "Losses: {'ner': 7.534278158197678}\n",
      "Losses: {'ner': 2.4968953206098723}\n",
      "Losses: {'ner': 3.734485207152737}\n",
      "Losses: {'ner': 5.882449314730945}\n",
      "Losses: {'ner': 5.882452114421479}\n",
      "Losses: {'ner': 6.038690023987694}\n",
      "Losses: {'ner': 0.0012797705744124078}\n",
      "Losses: {'ner': 0.0018025542763998004}\n",
      "Losses: {'ner': 2.118999060210503}\n",
      "Losses: {'ner': 3.5072142507485973}\n",
      "Losses: {'ner': 4.976329398611618}\n",
      "Losses: {'ner': 0.00749383957943445}\n",
      "Losses: {'ner': 2.003411088756385}\n",
      "Losses: {'ner': 2.004471942037081}\n",
      "Losses: {'ner': 3.488782025105826}\n",
      "Losses: {'ner': 3.527300407402044}\n",
      "Losses: {'ner': 7.609410040315726e-05}\n",
      "Losses: {'ner': 1.18543232759139}\n",
      "Losses: {'ner': 1.96331797689934}\n",
      "Losses: {'ner': 3.8990411130682365}\n",
      "Losses: {'ner': 3.9000820697636502}\n",
      "Losses: {'ner': 1.4436197105968884}\n",
      "Losses: {'ner': 3.2654442113820767}\n",
      "Losses: {'ner': 3.2654493715612936}\n",
      "Losses: {'ner': 3.2762540277898244}\n",
      "Losses: {'ner': 3.2762543150215433}\n",
      "Losses: {'ner': 0.00116152782799272}\n",
      "Losses: {'ner': 0.039124186671420334}\n",
      "Losses: {'ner': 1.6908728741652115}\n",
      "Losses: {'ner': 1.6948863984471916}\n",
      "Losses: {'ner': 3.2678567331914956}\n",
      "Losses: {'ner': 0.6929049946646982}\n",
      "Losses: {'ner': 0.6929565630515879}\n",
      "Losses: {'ner': 0.6930479709290183}\n",
      "Losses: {'ner': 2.45948607155435}\n",
      "Losses: {'ner': 2.459486073847451}\n",
      "Losses: {'ner': 1.1036269694917575}\n",
      "Losses: {'ner': 5.28369848250936}\n",
      "Losses: {'ner': 5.283698794128756}\n",
      "Losses: {'ner': 5.2836989160252115}\n",
      "Losses: {'ner': 5.283784192259767}\n",
      "Losses: {'ner': 1.9981863466705823}\n",
      "Losses: {'ner': 2.1376983579061637}\n",
      "Losses: {'ner': 2.137700895621598}\n",
      "Losses: {'ner': 2.5128124482504783}\n",
      "Losses: {'ner': 2.512812635904335}\n",
      "Losses: {'ner': 5.716221146918602e-05}\n",
      "Losses: {'ner': 1.0084976725659072}\n",
      "Losses: {'ner': 3.240809977649093}\n",
      "Losses: {'ner': 3.2408103107295827}\n",
      "Losses: {'ner': 3.2432500509217563}\n",
      "Losses: {'ner': 2.25466692262579e-09}\n",
      "Losses: {'ner': 1.9420048177181488}\n",
      "Losses: {'ner': 1.942069552623132}\n",
      "Losses: {'ner': 3.8349843450616166}\n",
      "Losses: {'ner': 3.8349843470990805}\n",
      "Losses: {'ner': 0.1879844517883251}\n",
      "Losses: {'ner': 0.19151788307976972}\n",
      "Losses: {'ner': 0.19159242240074675}\n",
      "Losses: {'ner': 0.19180559293439745}\n",
      "Losses: {'ner': 1.0793980942803463}\n",
      "Losses: {'ner': 0.009382425931104516}\n",
      "Losses: {'ner': 0.02808519583057725}\n",
      "Losses: {'ner': 0.028094708002734857}\n",
      "Losses: {'ner': 0.028308287715871728}\n",
      "Losses: {'ner': 1.0010809824535665}\n",
      "Losses: {'ner': 0.00021339762486295016}\n",
      "Losses: {'ner': 0.000525755230705358}\n",
      "Losses: {'ner': 0.0010495986905068074}\n",
      "Losses: {'ner': 2.3956786003082677}\n",
      "Losses: {'ner': 2.3956810901421717}\n",
      "Losses: {'ner': 0.37073400635683534}\n",
      "Losses: {'ner': 0.4216416290157482}\n",
      "Losses: {'ner': 0.4216416290162032}\n",
      "Losses: {'ner': 0.5113976630873718}\n",
      "Losses: {'ner': 1.4394657246302085}\n",
      "Losses: {'ner': 0.0007063962674306077}\n",
      "Losses: {'ner': 0.0007064171105218972}\n",
      "Losses: {'ner': 0.0007065508983262099}\n",
      "Losses: {'ner': 0.00284558329903696}\n",
      "Losses: {'ner': 0.0028902602462149854}\n",
      "Losses: {'ner': 7.869294592839533e-11}\n",
      "Losses: {'ner': 1.2777916583266273e-05}\n",
      "Losses: {'ner': 1.2777999536296137e-05}\n",
      "Losses: {'ner': 1.827168067048499e-05}\n",
      "Losses: {'ner': 0.28871234817296687}\n",
      "Losses: {'ner': 0.0004324408335409476}\n",
      "Losses: {'ner': 0.006745620763276701}\n",
      "Losses: {'ner': 0.045416836422286826}\n",
      "Losses: {'ner': 0.04555042415777528}\n",
      "Losses: {'ner': 0.04593445636370091}\n",
      "Losses: {'ner': 1.71376602090037e-05}\n",
      "Losses: {'ner': 0.004175623825263313}\n",
      "Losses: {'ner': 0.008823138562357113}\n",
      "Losses: {'ner': 0.008824641709935778}\n",
      "Losses: {'ner': 0.008824641710963269}\n",
      "Losses: {'ner': 1.890889310901691e-07}\n",
      "Losses: {'ner': 2.0177203991400273e-07}\n",
      "Losses: {'ner': 2.430940164845123e-07}\n",
      "Losses: {'ner': 2.711493743050263e-07}\n",
      "Losses: {'ner': 2.717531716543876e-07}\n",
      "Losses: {'ner': 6.3839564051797815e-09}\n",
      "Losses: {'ner': 7.781083020954267e-09}\n",
      "Losses: {'ner': 2.638463168922597e-07}\n",
      "Losses: {'ner': 2.9580409809218596e-07}\n",
      "Losses: {'ner': 0.061738858079904015}\n",
      "Losses: {'ner': 1.807229042054848}\n",
      "Losses: {'ner': 1.8072420314795274}\n",
      "Losses: {'ner': 1.8074106611523604}\n",
      "Losses: {'ner': 1.8074106614890508}\n",
      "Losses: {'ner': 1.8074106615728656}\n",
      "Losses: {'ner': 1.1814688624975285e-09}\n",
      "Losses: {'ner': 1.1562013045933708e-05}\n",
      "Losses: {'ner': 1.1563367026870514e-05}\n",
      "Losses: {'ner': 0.0022503147975683827}\n",
      "Losses: {'ner': 0.0022504514697417946}\n",
      "Losses: {'ner': 0.0008817544674556279}\n",
      "Losses: {'ner': 0.0008819039964627724}\n",
      "Losses: {'ner': 0.0008820324005867561}\n",
      "Losses: {'ner': 0.0256125136229559}\n",
      "Losses: {'ner': 0.025612561984873092}\n"
     ]
    }
   ],
   "source": [
    "# Training for 30 iterations\n",
    "for iterations in range(30):\n",
    "\n",
    "    # shuffling examples before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    #batch up the examples using spaCy's minibatch\n",
    "    for batch in spacy.util.minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001)):\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Update the model\n",
    "            nlp.update(\n",
    "                [example],\n",
    "                losses=losses,\n",
    "                drop=0.5\n",
    "        )\n",
    "        print(f\"Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Обучение новой категории сущности в spaCy\n",
    "В прошлых секциях, мы видели как `ner` категоризирует слова корректно. Но что, если вы хотите ввести категорию сущности, которой нет в текущей модели? \n",
    "\n",
    "Например, у вас есть много тестовых данных о еде, потребляемой в различных областях. И вы хотите, чтобы NER классифицировал все продукту питания по категории `FOOD`. Но такой категории не существует. Что делать?\n",
    "\n",
    "spaCy очень гибок и позволяет вам добавить новый вид сущности и обучить модель. Эта функция чрезвычайно полезно и позволяет вам легко добавить новый тип для лучшего поиска информации. Эта секция расскажет, как осуществить это.\n",
    "\n",
    "Для начала, загрущите предобученную модель с компонентом `ner`, используя `get_pipe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and load the spacy model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Getting the ner component\n",
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее вложите новую категорию сущности в переменную `LABEL`. Теперь, как модель узнает какие объекты стоит классифицировать новой категорией? Вы обучите ее на новых примерах. Примеры должны научить определять сущности класса `FOOD`. Подготовим тренировочные данные.\n",
    "\n",
    "Формат данных такой же как и раньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь набор данных готов, мы можем идти дальше и увидеть как эти примеры обучат компонент `ner`.\n",
    "\n",
    "Помните, что модель еще не знает, что такое `FOOD`.\n",
    "\n",
    "Так, что нашей первой задачей будет добавить лэйбл к `ner` с помощью `add_label()`. Далее вы можете использовать `resume_training()`\n",
    "\n",
    "Также, когда обучение завершится другие компоненты pipeline будут изменены. Для избежения этого, используйте `disable_pipes()`, чтобы отключить другие компоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses: {'ner': 1.0264057287137955e-14}\n",
      "Losses: {'ner': 5.2657296632916986e-11}\n",
      "Losses: {'ner': 5.2694514987082524e-11}\n",
      "Losses: {'ner': 5.269451505774786e-11}\n",
      "Losses: {'ner': 1.2674101764767096e-09}\n",
      "Losses: {'ner': 1.271050526296091e-09}\n",
      "Losses: {'ner': 1.286170666763914e-09}\n",
      "Losses: {'ner': 1.28617877503987e-09}\n",
      "Losses: {'ner': 1.286542526533046e-09}\n",
      "Losses: {'ner': 1.2865447433613305e-09}\n",
      "Losses: {'ner': 1.2976106841284021e-09}\n",
      "Losses: {'ner': 1.2976107057646817e-09}\n",
      "Losses: {'ner': 1.297647801246566e-09}\n",
      "Losses: {'ner': 1.3057176304633046e-09}\n",
      "Losses: {'ner': 9.898128249257072e-18}\n",
      "Losses: {'ner': 4.424195743635072e-12}\n",
      "Losses: {'ner': 5.57821306033705e-12}\n",
      "Losses: {'ner': 5.585800876052629e-12}\n",
      "Losses: {'ner': 5.588155277556233e-12}\n",
      "Losses: {'ner': 1.2107972516202681e-11}\n",
      "Losses: {'ner': 1.2465177338294161e-11}\n",
      "Losses: {'ner': 1.2498767274495398e-11}\n",
      "Losses: {'ner': 3.1326966673672224e-11}\n",
      "Losses: {'ner': 4.0633072958301925e-11}\n",
      "Losses: {'ner': 4.0634787895673995e-11}\n",
      "Losses: {'ner': 4.808772471280371e-11}\n",
      "Losses: {'ner': 5.3347166348447314e-11}\n",
      "Losses: {'ner': 5.334875955412416e-11}\n",
      "Losses: {'ner': 2.1039183126878376e-15}\n",
      "Losses: {'ner': 1.963547752925027e-14}\n",
      "Losses: {'ner': 1.1554547737063347e-08}\n",
      "Losses: {'ner': 1.155470963924971e-08}\n",
      "Losses: {'ner': 1.155983650394875e-08}\n",
      "Losses: {'ner': 1.1609339897619954e-08}\n",
      "Losses: {'ner': 1.1609501919049183e-08}\n",
      "Losses: {'ner': 1.1629419205740508e-08}\n",
      "Losses: {'ner': 1.1629513774914596e-08}\n",
      "Losses: {'ner': 1.1629513857093694e-08}\n",
      "Losses: {'ner': 1.1629531076938008e-08}\n",
      "Losses: {'ner': 1.1634141447803706e-08}\n",
      "Losses: {'ner': 1.16341418869052e-08}\n",
      "Losses: {'ner': 1.1634166772125953e-08}\n",
      "Losses: {'ner': 2.848608723221812e-18}\n",
      "Losses: {'ner': 1.6713449884219316e-16}\n",
      "Losses: {'ner': 2.851521567515443e-14}\n",
      "Losses: {'ner': 1.125509425244578e-10}\n",
      "Losses: {'ner': 1.1255196093028602e-10}\n",
      "Losses: {'ner': 1.2936316496441752e-10}\n",
      "Losses: {'ner': 1.2936335306398314e-10}\n",
      "Losses: {'ner': 1.2936562097441601e-10}\n",
      "Losses: {'ner': 1.2936570880269733e-10}\n",
      "Losses: {'ner': 1.294333919400634e-10}\n",
      "Losses: {'ner': 1.2944749697564674e-10}\n",
      "Losses: {'ner': 1.2944764152075094e-10}\n",
      "Losses: {'ner': 1.3118130869600805e-10}\n",
      "Losses: {'ner': 1.3119696475388104e-10}\n",
      "Losses: {'ner': 2.3451468080458354e-17}\n",
      "Losses: {'ner': 9.072962917018354e-13}\n",
      "Losses: {'ner': 9.08434612805821e-13}\n",
      "Losses: {'ner': 9.084546623983602e-13}\n",
      "Losses: {'ner': 9.084729147210736e-13}\n",
      "Losses: {'ner': 2.5237555049987205e-11}\n",
      "Losses: {'ner': 2.523757580988275e-11}\n",
      "Losses: {'ner': 2.558565998235419e-11}\n",
      "Losses: {'ner': 2.558576638501369e-11}\n",
      "Losses: {'ner': 2.5586986450027248e-11}\n",
      "Losses: {'ner': 2.558728464471647e-11}\n",
      "Losses: {'ner': 1.0278865524922223e-08}\n",
      "Losses: {'ner': 1.0278869910651986e-08}\n",
      "Losses: {'ner': 1.0278934389744168e-08}\n",
      "Losses: {'ner': 2.685710739892594e-13}\n",
      "Losses: {'ner': 2.697524982707405e-13}\n",
      "Losses: {'ner': 2.71243370851333e-13}\n",
      "Losses: {'ner': 2.8487957523320783e-13}\n",
      "Losses: {'ner': 2.8831075612017045e-13}\n",
      "Losses: {'ner': 8.225348701696614e-13}\n",
      "Losses: {'ner': 1.426495946504265e-12}\n",
      "Losses: {'ner': 1.4312829922996348e-12}\n",
      "Losses: {'ner': 6.180105405928212e-12}\n",
      "Losses: {'ner': 5.498200169782934e-10}\n",
      "Losses: {'ner': 5.498228836047472e-10}\n",
      "Losses: {'ner': 5.498262317362124e-10}\n",
      "Losses: {'ner': 5.498262339725351e-10}\n",
      "Losses: {'ner': 5.498317197979378e-10}\n",
      "Losses: {'ner': 6.755896806285826e-13}\n",
      "Losses: {'ner': 6.787842398370758e-13}\n",
      "Losses: {'ner': 6.883764299243062e-13}\n",
      "Losses: {'ner': 7.034273923752712e-13}\n",
      "Losses: {'ner': 1.2974197289371541e-12}\n",
      "Losses: {'ner': 1.306279733957326e-12}\n",
      "Losses: {'ner': 1.5131560229886004e-12}\n",
      "Losses: {'ner': 5.015850505911769e-10}\n",
      "Losses: {'ner': 5.034621598631869e-10}\n",
      "Losses: {'ner': 5.035766998673992e-10}\n",
      "Losses: {'ner': 5.035767025019438e-10}\n",
      "Losses: {'ner': 5.035767508256791e-10}\n",
      "Losses: {'ner': 5.041100432951821e-10}\n",
      "Losses: {'ner': 5.100768615175669e-10}\n",
      "Losses: {'ner': 1.0668403609804475e-11}\n",
      "Losses: {'ner': 1.0686399963071798e-11}\n",
      "Losses: {'ner': 1.0686410901960307e-11}\n",
      "Losses: {'ner': 1.0688270378839107e-11}\n",
      "Losses: {'ner': 1.0709551296541655e-11}\n",
      "Losses: {'ner': 4.654836031853324e-10}\n",
      "Losses: {'ner': 4.768418760154808e-10}\n",
      "Losses: {'ner': 4.778319387057225e-10}\n",
      "Losses: {'ner': 4.803812657857292e-10}\n",
      "Losses: {'ner': 4.80458649387842e-10}\n",
      "Losses: {'ner': 4.806149577416703e-10}\n",
      "Losses: {'ner': 4.91754457700016e-10}\n",
      "Losses: {'ner': 1.6163278920902977e-09}\n",
      "Losses: {'ner': 1.6163281831246993e-09}\n",
      "Losses: {'ner': 2.645808095342131e-16}\n",
      "Losses: {'ner': 2.457071076213253e-15}\n",
      "Losses: {'ner': 1.2628529754977168e-09}\n",
      "Losses: {'ner': 1.2630340912361591e-09}\n",
      "Losses: {'ner': 1.2630344562002815e-09}\n",
      "Losses: {'ner': 1.2653677315418415e-09}\n",
      "Losses: {'ner': 1.2657940454973936e-09}\n",
      "Losses: {'ner': 1.3624408241040722e-09}\n",
      "Losses: {'ner': 1.3624451626906876e-09}\n",
      "Losses: {'ner': 1.3669863992867812e-09}\n",
      "Losses: {'ner': 1.3669864046984828e-09}\n",
      "Losses: {'ner': 1.36698694748964e-09}\n",
      "Losses: {'ner': 1.3669870514126398e-09}\n",
      "Losses: {'ner': 1.3896233916463016e-09}\n",
      "Losses: {'ner': 1.6306181593595902e-13}\n",
      "Losses: {'ner': 4.0763036272606836e-13}\n",
      "Losses: {'ner': 2.373087514302068e-12}\n",
      "Losses: {'ner': 1.0987870277584959e-11}\n",
      "Losses: {'ner': 1.0988021647477262e-11}\n",
      "Losses: {'ner': 1.100521994883123e-11}\n",
      "Losses: {'ner': 1.109696818752852e-11}\n",
      "Losses: {'ner': 1.1308665987277e-11}\n",
      "Losses: {'ner': 1.2344004188968053e-11}\n",
      "Losses: {'ner': 1.2354167444949426e-11}\n",
      "Losses: {'ner': 1.2354167464563875e-11}\n",
      "Losses: {'ner': 1.2357342640062127e-11}\n",
      "Losses: {'ner': 1.2357355144175588e-11}\n",
      "Losses: {'ner': 1.4877245116250775e-11}\n",
      "Losses: {'ner': 2.204392412441877e-16}\n",
      "Losses: {'ner': 3.552060977852336e-11}\n",
      "Losses: {'ner': 3.5526335967672965e-11}\n",
      "Losses: {'ner': 8.426124244011501e-11}\n",
      "Losses: {'ner': 8.43521621317873e-11}\n",
      "Losses: {'ner': 1.0559059338597544e-10}\n",
      "Losses: {'ner': 1.0559073728047806e-10}\n",
      "Losses: {'ner': 1.0559073995294937e-10}\n",
      "Losses: {'ner': 1.1725849623800646e-10}\n",
      "Losses: {'ner': 2.0908732550087047e-10}\n",
      "Losses: {'ner': 2.0962441044130527e-10}\n",
      "Losses: {'ner': 2.0962442889254214e-10}\n",
      "Losses: {'ner': 2.096509931603176e-10}\n",
      "Losses: {'ner': 2.096560534491114e-10}\n",
      "Losses: {'ner': 5.781073058893179e-13}\n",
      "Losses: {'ner': 5.838075854473068e-13}\n",
      "Losses: {'ner': 5.840613437249096e-13}\n",
      "Losses: {'ner': 5.842193344337291e-13}\n",
      "Losses: {'ner': 5.845825433247249e-13}\n",
      "Losses: {'ner': 5.847294439783069e-13}\n",
      "Losses: {'ner': 5.884487453234121e-13}\n",
      "Losses: {'ner': 1.2949478168580686e-09}\n",
      "Losses: {'ner': 1.2949677189864427e-09}\n",
      "Losses: {'ner': 1.2988317970338419e-09}\n",
      "Losses: {'ner': 1.2988318038150183e-09}\n",
      "Losses: {'ner': 1.8574737698698562e-09}\n",
      "Losses: {'ner': 1.857473769959028e-09}\n",
      "Losses: {'ner': 1.8593149715656187e-09}\n",
      "Losses: {'ner': 6.786792438825444e-17}\n",
      "Losses: {'ner': 2.0938981525659956e-15}\n",
      "Losses: {'ner': 2.608326331267517e-13}\n",
      "Losses: {'ner': 2.613718715225483e-13}\n",
      "Losses: {'ner': 2.9250929288458513e-13}\n",
      "Losses: {'ner': 8.467405609386316e-11}\n",
      "Losses: {'ner': 8.514814521184913e-11}\n",
      "Losses: {'ner': 5.497447680842865e-09}\n",
      "Losses: {'ner': 5.497447709653786e-09}\n",
      "Losses: {'ner': 5.497453284005845e-09}\n",
      "Losses: {'ner': 5.4974591140219855e-09}\n",
      "Losses: {'ner': 5.497759696805169e-09}\n",
      "Losses: {'ner': 5.497759698440572e-09}\n",
      "Losses: {'ner': 5.549455931615046e-09}\n",
      "Losses: {'ner': 1.620019801591887e-15}\n",
      "Losses: {'ner': 6.99842824996693e-14}\n",
      "Losses: {'ner': 7.000257332772261e-14}\n",
      "Losses: {'ner': 8.790552515304864e-14}\n",
      "Losses: {'ner': 8.986819130673119e-14}\n",
      "Losses: {'ner': 6.80617753359603e-11}\n",
      "Losses: {'ner': 6.836292510763412e-11}\n",
      "Losses: {'ner': 6.836293134099628e-11}\n",
      "Losses: {'ner': 6.841063388162401e-11}\n",
      "Losses: {'ner': 6.845684779437556e-11}\n",
      "Losses: {'ner': 8.801254526332655e-11}\n",
      "Losses: {'ner': 8.842951881540858e-11}\n",
      "Losses: {'ner': 8.865088938719779e-11}\n",
      "Losses: {'ner': 8.865098494164636e-11}\n",
      "Losses: {'ner': 3.981853157355445e-13}\n",
      "Losses: {'ner': 1.4716020477031167e-11}\n",
      "Losses: {'ner': 1.473374925501943e-11}\n",
      "Losses: {'ner': 1.4738277871609352e-11}\n",
      "Losses: {'ner': 6.14350307411862e-10}\n",
      "Losses: {'ner': 6.143506255785652e-10}\n",
      "Losses: {'ner': 6.143507270893455e-10}\n",
      "Losses: {'ner': 6.143511783159316e-10}\n",
      "Losses: {'ner': 6.143511938907923e-10}\n",
      "Losses: {'ner': 3.012461307965369e-06}\n",
      "Losses: {'ner': 3.0124613085258493e-06}\n",
      "Losses: {'ner': 3.0124613085481135e-06}\n",
      "Losses: {'ner': 3.012461347720011e-06}\n",
      "Losses: {'ner': 3.012461348468413e-06}\n",
      "Losses: {'ner': 7.836544221285083e-14}\n",
      "Losses: {'ner': 2.0357794951713744e-11}\n",
      "Losses: {'ner': 2.3266457696647393e-11}\n",
      "Losses: {'ner': 3.5697996050571254e-11}\n",
      "Losses: {'ner': 4.4250715842020363e-11}\n",
      "Losses: {'ner': 7.30948148640527e-10}\n",
      "Losses: {'ner': 7.50249631809995e-10}\n",
      "Losses: {'ner': 7.502579108776871e-10}\n",
      "Losses: {'ner': 7.627244909383172e-10}\n",
      "Losses: {'ner': 7.628041656128035e-10}\n",
      "Losses: {'ner': 7.628044045835887e-10}\n",
      "Losses: {'ner': 7.633443771644523e-10}\n",
      "Losses: {'ner': 7.633642762409729e-10}\n",
      "Losses: {'ner': 7.730619835636401e-10}\n",
      "Losses: {'ner': 8.16748773106719e-11}\n",
      "Losses: {'ner': 8.167570135060698e-11}\n",
      "Losses: {'ner': 8.168264261612413e-11}\n",
      "Losses: {'ner': 8.168264316448087e-11}\n",
      "Losses: {'ner': 8.659163090878427e-11}\n",
      "Losses: {'ner': 8.786014304306688e-11}\n",
      "Losses: {'ner': 8.793930768890724e-11}\n",
      "Losses: {'ner': 8.843028153387108e-11}\n",
      "Losses: {'ner': 5.165013011778444e-10}\n",
      "Losses: {'ner': 5.165105610378108e-10}\n",
      "Losses: {'ner': 5.16510561044659e-10}\n",
      "Losses: {'ner': 5.165109200187645e-10}\n",
      "Losses: {'ner': 5.050858931122348e-09}\n",
      "Losses: {'ner': 5.0508795724721635e-09}\n",
      "Losses: {'ner': 1.0758923030261255e-18}\n",
      "Losses: {'ner': 8.198273453058957e-17}\n",
      "Losses: {'ner': 2.463886807999094e-10}\n",
      "Losses: {'ner': 2.4888378211098075e-10}\n",
      "Losses: {'ner': 2.488880265871742e-10}\n",
      "Losses: {'ner': 2.4888802685513724e-10}\n",
      "Losses: {'ner': 2.4888809620093824e-10}\n",
      "Losses: {'ner': 2.48888097633457e-10}\n",
      "Losses: {'ner': 8.021934996093854e-10}\n",
      "Losses: {'ner': 8.021942084882862e-10}\n",
      "Losses: {'ner': 2.4112513203486368e-09}\n",
      "Losses: {'ner': 2.41125907350497e-09}\n",
      "Losses: {'ner': 3.680699740573366e-09}\n",
      "Losses: {'ner': 3.6806997406540724e-09}\n",
      "Losses: {'ner': 5.298725347383001e-10}\n",
      "Losses: {'ner': 5.298956517032152e-10}\n",
      "Losses: {'ner': 5.298958995870813e-10}\n",
      "Losses: {'ner': 5.329496496277078e-10}\n",
      "Losses: {'ner': 5.351665711730894e-10}\n",
      "Losses: {'ner': 5.351678539531656e-10}\n",
      "Losses: {'ner': 7.896800935183284e-09}\n",
      "Losses: {'ner': 7.896905621797924e-09}\n",
      "Losses: {'ner': 7.896905679263706e-09}\n",
      "Losses: {'ner': 7.897561996430807e-09}\n",
      "Losses: {'ner': 7.897784816820855e-09}\n",
      "Losses: {'ner': 7.897784817175097e-09}\n",
      "Losses: {'ner': 8.172687268252434e-09}\n",
      "Losses: {'ner': 8.499738361318402e-09}\n",
      "Losses: {'ner': 4.896076792847983e-11}\n",
      "Losses: {'ner': 5.761552874599922e-11}\n",
      "Losses: {'ner': 5.8047181696232187e-11}\n",
      "Losses: {'ner': 5.808040659706141e-11}\n",
      "Losses: {'ner': 5.8145890106476556e-11}\n",
      "Losses: {'ner': 5.8148083235978905e-11}\n",
      "Losses: {'ner': 5.814808696053739e-11}\n",
      "Losses: {'ner': 5.817270568085344e-11}\n",
      "Losses: {'ner': 5.817427433773373e-11}\n",
      "Losses: {'ner': 5.8176342836004615e-11}\n",
      "Losses: {'ner': 5.8277401881387e-11}\n",
      "Losses: {'ner': 5.827741612997208e-11}\n",
      "Losses: {'ner': 5.827800844348077e-11}\n",
      "Losses: {'ner': 5.870496176969947e-11}\n",
      "Losses: {'ner': 3.0677516829459815e-17}\n",
      "Losses: {'ner': 7.462374882260937e-16}\n",
      "Losses: {'ner': 7.717833985631438e-16}\n",
      "Losses: {'ner': 8.851890841286448e-15}\n",
      "Losses: {'ner': 2.634966848577969e-13}\n",
      "Losses: {'ner': 4.251730442663769e-13}\n",
      "Losses: {'ner': 5.534024424072864e-13}\n",
      "Losses: {'ner': 5.617406907527133e-13}\n",
      "Losses: {'ner': 5.877086525577275e-13}\n",
      "Losses: {'ner': 4.348841481706552e-11}\n",
      "Losses: {'ner': 4.353636563445804e-11}\n",
      "Losses: {'ner': 4.3536461437787683e-11}\n",
      "Losses: {'ner': 4.3595955348724154e-11}\n",
      "Losses: {'ner': 4.7785217890238525e-11}\n",
      "Losses: {'ner': 1.3877401771043402e-16}\n",
      "Losses: {'ner': 2.642328198546528e-16}\n",
      "Losses: {'ner': 2.6490720170291784e-16}\n",
      "Losses: {'ner': 9.495035517603563e-15}\n",
      "Losses: {'ner': 1.267966580069357e-14}\n",
      "Losses: {'ner': 1.592625839305916e-14}\n",
      "Losses: {'ner': 1.7392726072765793e-14}\n",
      "Losses: {'ner': 4.4639870067988656e-14}\n",
      "Losses: {'ner': 4.5579561071944686e-14}\n",
      "Losses: {'ner': 4.55797249092022e-14}\n",
      "Losses: {'ner': 8.771196170471355e-14}\n",
      "Losses: {'ner': 1.5021086209679855e-10}\n",
      "Losses: {'ner': 1.5052039524875494e-10}\n",
      "Losses: {'ner': 1.5052050592270047e-10}\n",
      "Losses: {'ner': 2.976783951905525e-11}\n",
      "Losses: {'ner': 2.976819820829557e-11}\n",
      "Losses: {'ner': 2.9768198245438186e-11}\n",
      "Losses: {'ner': 2.977457661902116e-11}\n",
      "Losses: {'ner': 2.987022925248166e-11}\n",
      "Losses: {'ner': 2.987370576942366e-11}\n",
      "Losses: {'ner': 4.2746263844695234e-11}\n",
      "Losses: {'ner': 8.45802445984661e-11}\n",
      "Losses: {'ner': 8.458867600710285e-11}\n",
      "Losses: {'ner': 8.461429537450572e-11}\n",
      "Losses: {'ner': 8.534018441262788e-11}\n",
      "Losses: {'ner': 8.534029930933983e-11}\n",
      "Losses: {'ner': 9.524024376684204e-11}\n",
      "Losses: {'ner': 1.0170855872278773e-10}\n",
      "Losses: {'ner': 6.835183808625526e-11}\n",
      "Losses: {'ner': 6.886539603230626e-11}\n",
      "Losses: {'ner': 6.896384786251775e-11}\n",
      "Losses: {'ner': 6.896403786281985e-11}\n",
      "Losses: {'ner': 6.897809026650583e-11}\n",
      "Losses: {'ner': 6.957034132952998e-11}\n",
      "Losses: {'ner': 6.957385665878096e-11}\n",
      "Losses: {'ner': 6.95738569792647e-11}\n",
      "Losses: {'ner': 7.024828409535873e-11}\n",
      "Losses: {'ner': 7.024829412441182e-11}\n",
      "Losses: {'ner': 7.024830482149682e-11}\n",
      "Losses: {'ner': 7.025012259442598e-11}\n",
      "Losses: {'ner': 7.078404889900958e-11}\n",
      "Losses: {'ner': 7.106210922139143e-11}\n",
      "Losses: {'ner': 1.0900696792958671e-18}\n",
      "Losses: {'ner': 1.9477532884723653e-18}\n",
      "Losses: {'ner': 1.5946324300588195e-10}\n",
      "Losses: {'ner': 1.6268959455867393e-10}\n",
      "Losses: {'ner': 1.6269281524981192e-10}\n",
      "Losses: {'ner': 3.3310339770899684e-09}\n",
      "Losses: {'ner': 3.331075661405564e-09}\n",
      "Losses: {'ner': 3.3313452069179243e-09}\n",
      "Losses: {'ner': 3.3313501092721567e-09}\n",
      "Losses: {'ner': 3.3313570073548196e-09}\n",
      "Losses: {'ner': 3.3313591360425594e-09}\n",
      "Losses: {'ner': 3.3313591397365027e-09}\n",
      "Losses: {'ner': 5.174880074881101e-09}\n",
      "Losses: {'ner': 5.1860245957760655e-09}\n",
      "Losses: {'ner': 1.1837699303179199e-11}\n",
      "Losses: {'ner': 1.1840215077786581e-11}\n",
      "Losses: {'ner': 1.1844213192186045e-11}\n",
      "Losses: {'ner': 1.1969896136964856e-11}\n",
      "Losses: {'ner': 1.2015678213878716e-11}\n",
      "Losses: {'ner': 1.7458072302929692e-11}\n",
      "Losses: {'ner': 8.980391378325796e-10}\n",
      "Losses: {'ner': 8.980391707933929e-10}\n",
      "Losses: {'ner': 9.340657034755382e-10}\n",
      "Losses: {'ner': 9.383418138131933e-10}\n",
      "Losses: {'ner': 9.383528474885329e-10}\n",
      "Losses: {'ner': 9.384951108596857e-10}\n",
      "Losses: {'ner': 9.384953257810593e-10}\n",
      "Losses: {'ner': 9.387174831256792e-10}\n",
      "Losses: {'ner': 3.077050316192163e-09}\n",
      "Losses: {'ner': 3.0941152490566743e-09}\n",
      "Losses: {'ner': 3.0941200574918547e-09}\n",
      "Losses: {'ner': 3.094120343012461e-09}\n",
      "Losses: {'ner': 3.0941204274287712e-09}\n",
      "Losses: {'ner': 3.094120429374547e-09}\n",
      "Losses: {'ner': 3.099411026298989e-09}\n",
      "Losses: {'ner': 3.1009841639700675e-09}\n",
      "Losses: {'ner': 3.1009843212474128e-09}\n",
      "Losses: {'ner': 3.1010284795759225e-09}\n",
      "Losses: {'ner': 3.1010293802497115e-09}\n",
      "Losses: {'ner': 3.1067729204143647e-09}\n",
      "Losses: {'ner': 3.106772942995118e-09}\n",
      "Losses: {'ner': 3.106779172122637e-09}\n",
      "Losses: {'ner': 3.5305768197623726e-12}\n",
      "Losses: {'ner': 3.53099913922617e-12}\n",
      "Losses: {'ner': 3.5309997349407368e-12}\n",
      "Losses: {'ner': 4.42601445086785e-12}\n",
      "Losses: {'ner': 4.426300279251446e-12}\n",
      "Losses: {'ner': 4.428820287941462e-12}\n",
      "Losses: {'ner': 4.5645688202065044e-12}\n",
      "Losses: {'ner': 4.564726365342845e-12}\n",
      "Losses: {'ner': 4.565068038885918e-12}\n",
      "Losses: {'ner': 4.5680989836780145e-12}\n",
      "Losses: {'ner': 4.576520159803414e-12}\n",
      "Losses: {'ner': 4.592222503367781e-12}\n",
      "Losses: {'ner': 9.826106805736006e-12}\n",
      "Losses: {'ner': 9.827735355560419e-12}\n",
      "Losses: {'ner': 2.0689305435038372e-13}\n",
      "Losses: {'ner': 2.2028249124182437e-13}\n",
      "Losses: {'ner': 2.309450592500625e-13}\n",
      "Losses: {'ner': 3.4434026471693524e-10}\n",
      "Losses: {'ner': 3.443411696548047e-10}\n",
      "Losses: {'ner': 3.5254118844762284e-10}\n",
      "Losses: {'ner': 3.5254535413490326e-10}\n",
      "Losses: {'ner': 3.5256345700576653e-10}\n",
      "Losses: {'ner': 3.532699973287705e-10}\n",
      "Losses: {'ner': 3.532700041643615e-10}\n",
      "Losses: {'ner': 3.6094008845652687e-10}\n",
      "Losses: {'ner': 3.614988106427257e-10}\n",
      "Losses: {'ner': 3.6160547936956366e-10}\n",
      "Losses: {'ner': 3.616660155428582e-10}\n",
      "Losses: {'ner': 1.6812503952840027e-15}\n",
      "Losses: {'ner': 2.4197879088632102e-15}\n",
      "Losses: {'ner': 8.493701776857689e-12}\n",
      "Losses: {'ner': 8.494099061672175e-12}\n",
      "Losses: {'ner': 1.2687289133525258e-11}\n",
      "Losses: {'ner': 1.3168609163556585e-11}\n",
      "Losses: {'ner': 1.3168929686430486e-11}\n",
      "Losses: {'ner': 2.3543689386932515e-11}\n",
      "Losses: {'ner': 2.4153497336383612e-11}\n",
      "Losses: {'ner': 2.4161978413580912e-11}\n",
      "Losses: {'ner': 3.17450389840479e-10}\n",
      "Losses: {'ner': 3.175292896042874e-10}\n",
      "Losses: {'ner': 3.175293037904332e-10}\n",
      "Losses: {'ner': 4.984266189239337e-10}\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*other_pipes):\n",
    "    sizes = compounding(1.0, 3.0, 1.001)\n",
    "\n",
    "    for int in range(30):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in spacy.util.minibatch(TRAIN_DATA, size=sizes):\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                # Update the model\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    sgd = optimizer,\n",
    "                    losses=losses,\n",
    "                    drop=0.35\n",
    "            )\n",
    "            print(f\"Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "Entities [('Maggi', 'FOOD')]\n"
     ]
    }
   ],
   "source": [
    "# Tesing the NER\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('НИР_2-WuA8f2ou')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e6c9fab53fc3211fd37e0beae5e4569e950ef80c5ec8c35b734107b47e84f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
